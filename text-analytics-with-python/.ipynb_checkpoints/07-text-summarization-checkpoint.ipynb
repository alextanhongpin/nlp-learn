{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "What can we do?\n",
    "- extract key influential phrases from the documents\n",
    "- extract various diverse concepts or topics present in the documents\n",
    "- summarize the documents to provide a gists that retains the important parts of the whole corpus\n",
    "\n",
    "\n",
    "## Techniques\n",
    "\n",
    "- keyphrase extraction - extracting keywords or phrases from a text document of corpus that capture its main concepts or themes\n",
    "- topic modelling - using statistical and mathematical modelling techniques to extract main topics, themes or concepts from a corpus of documents.\n",
    "- automated document summarization - process of using a computer program or algorithm based on statistical and ML techniques to summarize a document or corpus of documents such that we obtain a short summary that captures all the essential concepts and themes of the original document or corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(document):\n",
    "    '''\n",
    "    Remove newline from document, parse the text, convert it into ASCII format, \n",
    "    and break down into its sentence constituents.\n",
    "    '''\n",
    "    document = re.sub(\"\\n\", ' ', document)\n",
    "    if isinstance(document, str):\n",
    "        document = document\n",
    "    elif isinstance(document, unicode):\n",
    "        return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore')\n",
    "    else:\n",
    "        raise ValueError('Document is not string or unicode!')\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "html_parser = HTMLParser()\n",
    "def unescape_html(parser, text):\n",
    "    return parser.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.contractions import expand_contractions\n",
    "from module.normalization import remove_special_characters, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, lemmatize=True, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text)\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "- binary term occurence-based features\n",
    "- frequency bag of words-based features\n",
    "- tf-idf weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency'):\n",
    "    feature_type = feature_type.lower().strip()\n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=1, ngram_range=(1, 1))\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=1, ngram_range=(1, 1))\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 1))\n",
    "    else:\n",
    "        raise Exception('Wrong feature type entered. Possible values: \"binary\", \"frequency\", \"tfidf\"')\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrase Extraction\n",
    "\n",
    "A.k.a terminology extraction, is defined as the process or technique of extracting key important and relevant terms or phrases from a body of unstructured text such that the core topics or themes of the text document(s) are captured in these key phrases.\n",
    "\n",
    "- semantic web\n",
    "- query-based search engine and crawlers\n",
    "- recommendation systems\n",
    "- tagging systems\n",
    "- document similarity\n",
    "- translation\n",
    "\n",
    "Techniques for keyphrase extraction:\n",
    "- collocations\n",
    "- weighted tag-based phrase extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation\n",
    "\n",
    "A collocation is a sequence or group of words that tend to occur frequently such that this frequency tends to be more than what could be termed as a random chance occurence. \n",
    "\n",
    "Techniques to extract collocations:\n",
    "- n-gram grouping or segmentation approach (construct ngrams out of a corpus, count the frequency of each ngram, and rank them based on their frequency of occurence to get the most frequent n-gram collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alice adventure wonderland lewis carroll 1865'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from module.normalization import normalize_corpus\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "\n",
    "# Load corpus.\n",
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "norm_alice = list(filter(None, normalize_corpus(alice, False)))\n",
    "\n",
    "# Print first line.\n",
    "norm_alice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip() \n",
    "                     for document in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams(sequence, n):\n",
    "    return zip(*[sequence[index:]\n",
    "                 for index in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 4)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(compute_ngrams([1,2,3,4], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (2, 3, 4)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(compute_ngrams([1,2,3,4], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),\n",
    "                              key=itemgetter(1),\n",
    "                              reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq)\n",
    "                     for text, freq in sorted_ngrams]\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say alice', 123),\n",
       " ('mock turtle', 56),\n",
       " ('march hare', 31),\n",
       " ('say king', 29),\n",
       " ('thought alice', 26),\n",
       " ('white rabbit', 22),\n",
       " ('say hatter', 22),\n",
       " ('say mock', 21),\n",
       " ('alice say', 19),\n",
       " ('say gryphon', 19)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 bigrams.\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=2, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say mock turtle', 21),\n",
       " ('say march hare', 10),\n",
       " ('poor little thing', 6),\n",
       " ('say alice say', 6),\n",
       " ('little golden key', 5),\n",
       " ('certainly say alice', 5),\n",
       " ('white kid glove', 5),\n",
       " ('march hare say', 5),\n",
       " ('mock turtle say', 5),\n",
       " ('know say alice', 4)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 trigrams.\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=3, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split()\n",
    "                                                 for item\n",
    "                                                 in norm_alice])\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say', 'alice'),\n",
       " ('mock', 'turtle'),\n",
       " ('march', 'hare'),\n",
       " ('say', 'king'),\n",
       " ('thought', 'alice'),\n",
       " ('say', 'hatter'),\n",
       " ('white', 'rabbit'),\n",
       " ('say', 'mock'),\n",
       " ('say', 'caterpillar'),\n",
       " ('say', 'gryphon')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw frequencies.\n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acceptance', 'elegant'),\n",
       " ('accustom', 'usurpation'),\n",
       " ('actually', 'took'),\n",
       " ('adjourn', 'immediate'),\n",
       " ('adoption', 'energetic'),\n",
       " ('affair', 'trust'),\n",
       " ('agony', 'terror'),\n",
       " ('ambition', 'distraction'),\n",
       " ('ancient', 'modern'),\n",
       " ('arithmetic', 'ambition')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pointwise mutual information.\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams.\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split()\n",
    "                                                  for item\n",
    "                                                  in norm_alice])\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say', 'mock', 'turtle'),\n",
       " ('say', 'march', 'hare'),\n",
       " ('poor', 'little', 'thing'),\n",
       " ('little', 'golden', 'key'),\n",
       " ('march', 'hare', 'say'),\n",
       " ('mock', 'turtle', 'say'),\n",
       " ('white', 'kid', 'glove'),\n",
       " ('beau', 'ootiful', 'soo'),\n",
       " ('certainly', 'say', 'alice'),\n",
       " ('might', 'well', 'say')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw frequencies.\n",
    "finder.nbest(trigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accustom', 'usurpation', 'conquest'),\n",
       " ('adjourn', 'immediate', 'adoption'),\n",
       " ('adoption', 'energetic', 'remedy'),\n",
       " ('ancient', 'modern', 'seaography'),\n",
       " ('arithmetic', 'ambition', 'distraction'),\n",
       " ('brother', 'latin', 'grammar'),\n",
       " ('crocodile', 'improve', 'shin'),\n",
       " ('crust', 'gravy', 'meat'),\n",
       " ('curve', 'graceful', 'zigzag'),\n",
       " ('elsie', 'lacie', 'tillie')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pointwise mutual information.\n",
    "finder.nbest(trigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Tag-Based Phrase Extraction\n",
    "\n",
    "1. extract all noun phrases chunks using shallow parsing\n",
    "2. compute tf-idf weights for each chunk and return the top weighted phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_text = \"\"\"Elephants are mammals of the family Elephantidae and the largest existing land animals. Three species are currently recognised: the African bush elephant, the African forest elephant, and the Asian elephant. Elephantidae is the only surviving family of the order Proboscidea; extinct members include the mastodons. The family Elephantidae also contains several now-extinct groups, including the mammoths and straight-tusked elephants. African elephants have larger ears and concave backs, whereas Asian elephants have smaller ears, and convex or level backs. Distinctive features of all elephants include a long trunk, tusks, large ear flaps, massive legs, and tough but sensitive skin. The trunk, also called a proboscis, is used for breathing, bringing food and water to the mouth, and grasping objects. Tusks, which are derived from the incisor teeth, serve both as weapons and as tools for moving objects and digging. The large ear flaps assist in maintaining a constant body temperature as well as in communication. The pillar-like legs carry their great weight.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.normalization import stopword_list\n",
    "import itertools\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}'):\n",
    "    # Build chunker based on grammar pattern.\n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # POS tag sentences.\n",
    "        tagged_sents = nltk.pos_tag_sents([nltk.word_tokenize(sentence)])\n",
    "        \n",
    "        # Extract chunks.\n",
    "        chunks = [chunker.parse(tagged_sent)\n",
    "                  for tagged_sent in tagged_sents]\n",
    "        \n",
    "        \n",
    "        # Get word, pos_tag, chunk tag triples.\n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                     for chunk in chunks]\n",
    "        \n",
    "        flattened_chunks = list(itertools.chain.from_iterable(wtc_sent for wtc_sent in wtc_sents))\n",
    "        \n",
    "        # Get valid chunks based on tags.\n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk])\n",
    "                               for status, chunk\n",
    "                               in itertools.groupby(flattened_chunks,\n",
    "                                                    lambda word_pos_chunk: word_pos_chunk[2] != 'O')]\n",
    "        \n",
    "        # Append words in each chunk to make phrases.\n",
    "        valid_chunks = [' '.join(word.lower() \n",
    "                                 for word, tag, chunk\n",
    "                                 in wtc_group\n",
    "                                     if word.lower() \n",
    "                                         not in stopword_list)\n",
    "                                for status, wtc_group\n",
    "                                in valid_chunks_tagged \n",
    "                                    if status]\n",
    "        \n",
    "        # Append all valid chunked phrases.\n",
    "        all_chunks.append(valid_chunks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elephants are mammals of the family Elephantidae and the largest existing land animals.',\n",
       " 'Three species are currently recognised: the African bush elephant, the African forest elephant, and the Asian elephant.',\n",
       " 'Elephantidae is the only surviving family of the order Proboscidea; extinct members include the mastodons.',\n",
       " 'The family Elephantidae also contains several now-extinct groups, including the mammoths and straight-tusked elephants.',\n",
       " 'African elephants have larger ears and concave backs, whereas Asian elephants have smaller ears, and convex or level backs.',\n",
       " 'Distinctive features of all elephants include a long trunk, tusks, large ear flaps, massive legs, and tough but sensitive skin.',\n",
       " 'The trunk, also called a proboscis, is used for breathing, bringing food and water to the mouth, and grasping objects.',\n",
       " 'Tusks, which are derived from the incisor teeth, serve both as weapons and as tools for moving objects and digging.',\n",
       " 'The large ear flaps assist in maintaining a constant body temperature as well as in communication.',\n",
       " 'The pillar-like legs carry their great weight.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = parse_document(toy_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['elephants', 'mammals', 'family elephantidae', 'land animals'],\n",
       " ['species',\n",
       "  'african bush elephant',\n",
       "  'african forest elephant',\n",
       "  'asian elephant'],\n",
       " ['elephantidae',\n",
       "  'family',\n",
       "  'order proboscidea',\n",
       "  'extinct members',\n",
       "  'mastodons'],\n",
       " ['family elephantidae',\n",
       "  'several now-extinct groups',\n",
       "  'mammoths',\n",
       "  'straight-tusked elephants'],\n",
       " ['african elephants',\n",
       "  'ears',\n",
       "  'backs',\n",
       "  'whereas asian elephants',\n",
       "  'ears',\n",
       "  'convex',\n",
       "  'level backs'],\n",
       " ['distinctive features',\n",
       "  'elephants',\n",
       "  'long trunk',\n",
       "  'tusks',\n",
       "  'large ear flaps',\n",
       "  'massive legs',\n",
       "  'sensitive skin'],\n",
       " ['trunk', 'proboscis', 'breathing', 'food', 'water', 'mouth', 'objects'],\n",
       " ['tusks', 'incisor teeth', 'weapons', 'tools', 'objects', 'digging'],\n",
       " ['large ear flaps', 'constant body temperature', 'communication'],\n",
       " ['pillar-like legs', 'great weight']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all valid keyphrases per sentence of our document. Since we targetted nouns, all phrases talk about noun based entities.\n",
    "valid_chunks = get_chunks(sentences)\n",
    "valid_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_weighted_keyphrases(sentences, \n",
    "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "                                  top_n=10):\n",
    "    # Get valid chunks.\n",
    "    valid_chunks = get_chunks(sentences, grammar)\n",
    "    \n",
    "    # Build tf-idf based model.\n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    \n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    # Get phrases and their tf-idf weights.\n",
    "    weighted_phrases = {dictionary.get(id): round(value, 3)\n",
    "                        for doc in corpus_tfidf\n",
    "                        for id, value in doc}\n",
    "    weighted_phrases = sorted(weighted_phrases.items(), \n",
    "                              key=itemgetter(1), \n",
    "                              reverse=True)\n",
    "\n",
    "    # Return top weighted phrases.\n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great weight', 0.707),\n",
       " ('pillar-like legs', 0.707),\n",
       " ('ears', 0.667),\n",
       " ('communication', 0.634),\n",
       " ('constant body temperature', 0.634),\n",
       " ('land animals', 0.58),\n",
       " ('mammals', 0.58),\n",
       " ('mammoths', 0.535),\n",
       " ('several now-extinct groups', 0.535),\n",
       " ('straight-tusked elephants', 0.535)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf_weighted_keyphrases(sentences, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
