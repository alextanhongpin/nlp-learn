{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification/categorization\n",
    "\n",
    "    What is text classification?\n",
    "\n",
    "Text classification is the process of assigning text documents into one or more classes or categories, assuming that we have a predefined set of classes.\n",
    "\n",
    "Documents here are textual documents, and each document can contain a sentence or even a paragraph of words. \n",
    "\n",
    "## Two types of text classification\n",
    "\n",
    "    What types of text classifications are available?\n",
    "\n",
    "- content-based classification\n",
    "- request-based classification\n",
    "\n",
    "__Content-based classification__ is the type of text classification where priorities or weights are given to a specific subjects or topics in the text content that would help determine the class of the document.\n",
    "\n",
    "E.g., a book with more than 30 percent of its content about food preparations can be classified under cooking/recipes. \n",
    "\n",
    "__Request-based classification__ is influenced by user requests and targeted towards specific user groups and audiences. This type of classification is governed by specific policies and ideals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification blueprint\n",
    "\n",
    "1. prepare test, train and validation (optional) datasets\n",
    "2. text normalization\n",
    "3. feature extraction\n",
    "4. model training\n",
    "5. model prediction and evaluation\n",
    "6. model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "\n",
    "- expanding contractions\n",
    "- text standardization through lemmatization\n",
    "- removing special characters and aymbols\n",
    "- removing stopwords\n",
    "\n",
    "Others:\n",
    "- correcting spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use modules, create a directory module and a __init__.py file there.\n",
    "# Note that a .py file cannot be in the same folder as the .ipynb, else it will throw an exception.\n",
    "from module.contractions import expand_contractions \n",
    "from module.tokenize import tokenize_text\n",
    "from module.lemmatize import lemmatize_text, pos_tag_text\n",
    "# from module.feature_extractor import bow_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not good'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"this isn't good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define function to tokenize text into tokens that will be used by our other normalization functions.\n",
    "tokenize_text('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[hello] world'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Match any hello.\n",
    "pattern = re.compile('hello')\n",
    "\n",
    "# Define a substitution function that allows us access to the matched word.\n",
    "def subfn(m):\n",
    "    match = m.group(0)\n",
    "    return f'[{match}]'\n",
    "    \n",
    "pattern.sub(subfn, 'hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where be you play football'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text('where are you playing football')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) \n",
    "                                    for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky blue',\n",
       " ['sky', 'blue'],\n",
       " 'sky blue sky beautiful',\n",
       " ['sky', 'blue', 'sky', 'beautiful'],\n",
       " 'beautiful sky blue',\n",
       " ['beautiful', 'sky', 'blue'],\n",
       " 'love blue cheese',\n",
       " ['love', 'blue', 'cheese']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus(CORPUS, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "\n",
    "### What is feature extraction/engineering?\n",
    "    \n",
    "- The process of extracting and selecting features\n",
    "\n",
    "### What is feature?\n",
    "\n",
    "- features are unique, measurable attributes or properties for each observation or data point in a dataset.\n",
    "- features are usuallu numeric in nature and can be absolute numeric values or categorical features that can be encoded as binary features for each category in the list using a process called __one-hot encoding__.\n",
    "\n",
    "### What are examples of feature extraction techniques?\n",
    "\n",
    "- bag of words model\n",
    "- tf-idf model\n",
    "- advanced word vectorization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Bag of Words\n",
    "\n",
    "Disadvantage:\n",
    "- vectors are completely based on the absolute frequencies of word occurences\n",
    "- this may have potential problems where words that may tend to occur a lot across all documents in the corpus will have higher frequencies and will tend to overshadow other words that may not occur as frequently but may be more interesting and effective as features to identify specific categories for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bow_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 1, 0, 1, 1],\n",
       "        [1, 1, 1, 0, 2, 0, 2, 0],\n",
       "        [0, 1, 1, 0, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build bow vectorizer and get features.\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features from new document using built vectorizer.\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "new_doc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'the']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the feature names.\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  the\n",
      "0    0          0     1       0   1     0    1    1\n",
      "1    1          1     1       0   2     0    2    0\n",
      "2    0          1     1       0   1     0    1    1\n",
      "3    0          0     1       1   0     1    0    0\n"
     ]
    }
   ],
   "source": [
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  the\n",
      "0    0          0     1       0   0     0    1    0\n"
     ]
    }
   ],
   "source": [
    "display_features(new_doc_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: TF-IDF Model\n",
    "\n",
    "- product of two metrics, term frequency (tf) and inverse document frequency (idf)\n",
    "- term frequency is the raw frequency value of that term in a particular document\n",
    "- $tf(w, D) = f_\\text(wD')$, $f_\\text(wD')$ denotes frequency for word in document D\n",
    "- inverse document frequency is the inverse of the document frequency for each term.\n",
    "- idf is computed by dividing the total number of documents in our corpus by the document frequency for each term and then applying logarithmic scaling on the result\n",
    "\n",
    "We add 1 to the document frequency for each term to indicate that we have one more document in our corpus that essentially has every term in the vocabulary. This is to prevent potential division-by-zero errors and smoothen the inverse document frequencies. We also add 1 to our result of our idf to avoid ignoring terms completely that might have zero idf:\n",
    "\n",
    "$idf(t) = 1 + log\\frac{C}{1 + df(t)}$\n",
    "\n",
    "Where:\n",
    "- $C$ is the count of the total number of documents in our corpus\n",
    "- $idf(t)$ is the idf for term t\n",
    "- $df(t)$ is the frequency of the number of documents in which term t is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00\n",
      "2  0.00       0.52  0.34    0.00  0.42  0.00  0.42  0.52\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build tfidf transformer and show train corpus tfidf features.\n",
    "tfidf_trans, tfidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tfidf_features.todense(), 2)\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0\n"
     ]
    }
   ],
   "source": [
    "# Show tfidf features for new_doc using built tfidf transformer.\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing TF-IDF from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "# Compute term frequency.\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# Show term frequency.\n",
    "display_features(tf, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the document frequency matrix.\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # To smoothen the idf later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  the\n",
      "0    2          3     5       2   4     2    4    3\n"
     ]
    }
   ],
   "source": [
    "# How many times the term appear in each document + 1.\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inverse document frequencies.\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.51\n"
     ]
    }
   ],
   "source": [
    "# Show inverse document frequencies.\n",
    "display_features([np.round(idf, 2)], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute idf diagonal matrix.\n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.92, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.51, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.92, 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 1.22, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 1.92, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.22, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.51]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(idf, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "tfidf = tf * idf\n",
    "display_features(np.round(tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2 norms.\n",
    "norms = norm(tfidf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5 , 4.35, 2.93, 2.89])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print norms for each document.\n",
    "np.round(norms, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.39921021, 0.        , 0.48829139,\n",
       "         0.        , 0.48829139, 0.60313701],\n",
       "        [0.44051607, 0.34730793, 0.22987956, 0.        , 0.5623514 ,\n",
       "         0.        , 0.5623514 , 0.        ],\n",
       "        [0.        , 0.51646957, 0.34184591, 0.        , 0.41812662,\n",
       "         0.        , 0.41812662, 0.51646957],\n",
       "        [0.        , 0.        , 0.34618161, 0.66338461, 0.        ,\n",
       "         0.66338461, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute normalized tfidf.\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "norm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00\n",
      "2  0.00       0.52  0.34    0.00  0.42  0.00  0.42  0.52\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# Show final tfidf feature matrix.\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new doc terms freqs from bow freqs.\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n",
    "\n",
    "# Compute tfidf using idf matrix from train corpus.\n",
    "nd_tfidf = nd_tf * idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0\n"
     ]
    }
   ],
   "source": [
    "# Show new_doc tfidf feature vector.\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00\n",
      "2  0.00       0.52  0.34    0.00  0.42  0.00  0.42  0.52\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# Build tfidf vectorizer and get training corpus feature vectors.\n",
    "tfidf_vectorizer, tfidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tfidf_features.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0\n"
     ]
    }
   ],
   "source": [
    "# Get tfidf feature vector for the new document.\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
